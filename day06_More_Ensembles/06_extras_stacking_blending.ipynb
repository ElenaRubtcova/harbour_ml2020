{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06 Extras: Stacking & Blending Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook contents were partially inspired by <a href=\"https://alexanderdyakonov.wordpress.com/2017/03/10/cтекинг-stacking-и-блендинг-blending/\"> Alexander Diakonov Blog </a>*`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's have a short recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging:\n",
    "\n",
    "* Why Decision Trees are good to be used for bagging?\n",
    "* How to estimate feature importances using the contructed Random Forest?\n",
    "* How out-of-bag error is calculated using the constructed Random Forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice: use Blending to finish EDA practice on Titanic Kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = 10, 6\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV, train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('data/titanic_dataset/train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat the Data Pre-processing step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Initials for people's names in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets extract the Salutations\n",
    "data['Initial']=0\n",
    "for i in data:\n",
    "    data['Initial']=data.Name.str.extract('([A-Za-z]+)\\.')\n",
    "\n",
    "data['Initial'].replace(\n",
    "    ['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],\n",
    "    ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],\n",
    "    inplace=True)\n",
    "data.groupby('Initial')['Age'].mean() #lets check the average age by Initials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill NaNs in `Age` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assigning the NaN Values with the Ceil values of the mean ages\n",
    "data.loc[(data.Age.isnull())&(data.Initial=='Mr'),'Age']=33\n",
    "data.loc[(data.Age.isnull())&(data.Initial=='Mrs'),'Age']=36\n",
    "data.loc[(data.Age.isnull())&(data.Initial=='Master'),'Age']=5\n",
    "data.loc[(data.Age.isnull())&(data.Initial=='Miss'),'Age']=22\n",
    "data.loc[(data.Age.isnull())&(data.Initial=='Other'),'Age']=46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Age.isnull().any() #So no null values left finally "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill NaNs for `Embarked` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Embarked'].fillna('S',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally No NaN values\n",
    "data.Embarked.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat the Feature Engineering step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert age values to age bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Age_band']=0\n",
    "data.loc[data['Age']<=16,'Age_band']=0\n",
    "data.loc[(data['Age']>16)&(data['Age']<=32),'Age_band']=1\n",
    "data.loc[(data['Age']>32)&(data['Age']<=48),'Age_band']=2\n",
    "data.loc[(data['Age']>48)&(data['Age']<=64),'Age_band']=3\n",
    "data.loc[data['Age']>64,'Age_band']=4\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add `FamilySize` and `Alone` features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# family size\n",
    "data['Family_Size']=0\n",
    "data['Family_Size']=data['Parch']+data['SibSp']\n",
    "# aloneb\n",
    "data['Alone']=0\n",
    "data.loc[data.Family_Size==0,'Alone']=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add `FareCat` for fare categories, based on fare distribution quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Fare_Range'] = pd.qcut(data['Fare'],4)\n",
    "data.groupby(['Fare_Range'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Fare_cat']=0\n",
    "data.loc[data['Fare']<=7.91,'Fare_cat']=0\n",
    "data.loc[(data['Fare']>7.91)&(data['Fare']<=14.454),'Fare_cat']=1\n",
    "data.loc[(data['Fare']>14.454)&(data['Fare']<=31),'Fare_cat']=2\n",
    "data.loc[(data['Fare']>31)&(data['Fare']<=513),'Fare_cat']=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covnert string columns into numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Sex'].replace(['male','female'],[0,1],inplace=True)\n",
    "data['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)\n",
    "data['Initial'].replace(['Mr','Mrs','Miss','Master','Other'],[0,1,2,3,4],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Name','Age','Ticket','Fare','Cabin','Fare_Range','PassengerId'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictive Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=train_test_split(data, test_size=0.2, random_state=42, stratify=data['Survived'])\n",
    "\n",
    "train_X=train[train.columns[1:]]\n",
    "train_Y=train[train.columns[:1]]\n",
    "\n",
    "test_X=test[test.columns[1:]]\n",
    "test_Y=test[test.columns[:1]]\n",
    "\n",
    "X=data[data.columns[1:]]\n",
    "Y=data['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a logistic regression baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5)\n",
    "baseline_model.fit(train_X, train_Y)\n",
    "prediction_bl = baseline_model.predict(test_X)\n",
    "print('The ROC-AUC of the Logistic Regression is',metrics.roc_auc_score(prediction_bl, test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the required ML packages\n",
    "from sklearn.linear_model import LogisticRegression    #logistic regression\n",
    "from sklearn.ensemble import RandomForestClassifier    #Random Forest\n",
    "from sklearn.neighbors import KNeighborsClassifier     #KNN\n",
    "from sklearn.tree import DecisionTreeClassifier        #Decision Tree\n",
    "from sklearn.model_selection import train_test_split   #training and testing data split\n",
    "from sklearn import metrics                            #accuracy measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold #for K-fold cross validation\n",
    "from sklearn.model_selection import cross_val_score #score evaluation\n",
    "from sklearn.model_selection import cross_val_predict #prediction\n",
    "from sklearn.model_selection import GridSearchCV # grid search\n",
    "\n",
    "kfold = KFold(n_splits=3, random_state=22)\n",
    "\n",
    "xyz = []\n",
    "roc_values_cv, std = [], []\n",
    "roc_values_test = []\n",
    "classifiers=['Logistic Regression', 'KNN', 'Decision Tree', 'Random Forest']\n",
    "models=[\n",
    "    LogisticRegression(C=1e2),\n",
    "    KNeighborsClassifier(n_neighbors=9),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(n_estimators=30, max_depth=5),\n",
    "]\n",
    "\n",
    "for i in models:\n",
    "    model = i\n",
    "    cv_result = cross_val_score(model, train_X, train_Y, cv=kfold, scoring=\"roc_auc\")\n",
    "    cv_result = cv_result\n",
    "    xyz.append(cv_result.mean())\n",
    "    std.append(cv_result.std())\n",
    "    roc_values_cv.append(cv_result)\n",
    "    model.fit(train_X, train_Y)\n",
    "    roc_auc_test = metrics.roc_auc_score(model.predict(test_X), test_Y)\n",
    "    roc_values_test.append(roc_auc_test)\n",
    "    \n",
    "new_models_dataframe2=pd.DataFrame({'CV Mean':xyz,'Std':std, 'Test': roc_values_test} ,index=classifiers)       \n",
    "new_models_dataframe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,6))\n",
    "box=pd.DataFrame(roc_values_cv,index=[classifiers])\n",
    "box.T.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_models_dataframe2['CV Mean'].plot.barh(width=0.8)\n",
    "plt.title('Average CV Mean Accuracy')\n",
    "fig=plt.gcf()\n",
    "fig.set_size_inches(8,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking and Blending Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is split into N parts (folds), then the base algorithms are iteratively fit to N-1 folds and generate predictions for the N-th fold. Those predictions are then treated as the new features for the given N-th fold. To generate such metafeatures for the testing set, base algorithms are fitted to the full training dataset instead.\n",
    "\n",
    "<img src='pics/stacking-eng.png'>\n",
    "\n",
    "Simple way of stacking - blending:\n",
    "1. Split training set into two disjoint subsets\n",
    "2. Train several base models using the first subset of the data\n",
    "3. Generate predictions for the second subset using the trained base models, and vice versa\n",
    "4. Using the predictions from 3. as features and the ground truth targets, train a classifier using such metafeatures (secod-level classifier)\n",
    "\n",
    "<img src='pics/blending-eng.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for repeatability\n",
    "SEED = 10\n",
    "\n",
    "# number of folds for out-of-fold predictions\n",
    "NFOLDS = 3\n",
    "kf = KFold(n_splits=NFOLDS, random_state=SEED)\n",
    "\n",
    "train_X, train_Y = np.array(train_X), np.array(train_Y)\n",
    "test_X, test_Y = np.array(test_X), np.array(test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof(clf, x_train, y_train, x_test):\n",
    "    ntrain = x_train.shape[0]\n",
    "    ntest = x_test.shape[0]\n",
    "    \n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(kf.split(x_train, y_train)):\n",
    "        x_tr = x_train[train_index, :]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index, :]\n",
    "\n",
    "        clf.fit(x_tr, y_tr)\n",
    "        \n",
    "        oof_train[test_index] = clf.predict_proba(x_te)[:, 1]\n",
    "        oof_test_skf[i, :] = clf.predict_proba(x_test)[:, 1]\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression(random_state=SEED)\n",
    "model_knn = KNeighborsClassifier(n_neighbors=9)\n",
    "model_dt = DecisionTreeClassifier(max_depth=5)\n",
    "model_rf = RandomForestClassifier(n_estimators=100, random_state=SEED, max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our OOF train and test predictions. These base results will be used as new features\n",
    "lr_oof_train, lr_oof_test = get_oof(model_lr, train_X, train_Y, test_X) # Logistic Regression\n",
    "knn_oof_train, knn_oof_test = get_oof(model_knn, train_X, train_Y, test_X) # KNN\n",
    "dt_oof_train, dt_oof_test = get_oof(model_dt, train_X, train_Y, test_X) # XGBoost\n",
    "rf_oof_train, rf_oof_test = get_oof(model_rf, train_X, train_Y, test_X) # Random Forest\n",
    "print(\"Training is complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_predictions_train = pd.DataFrame(\n",
    "    {\n",
    "        'LogReg': lr_oof_train.ravel(),\n",
    "        'kNN': knn_oof_train.ravel(),\n",
    "        'DecisionTree': dt_oof_train.ravel(),\n",
    "        'RandomForest': rf_oof_train.ravel(),\n",
    "        'GT': np.array(train_Y).ravel()\n",
    "    }\n",
    ")\n",
    "base_predictions_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate((lr_oof_train, knn_oof_train, dt_oof_train, rf_oof_train), axis=1)\n",
    "x_test = np.concatenate((lr_oof_test, knn_oof_test, dt_oof_test, rf_oof_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb_model = XGBClassifier()\n",
    "\n",
    "# Create the parameter grid\n",
    "gbm_param_grid = {\n",
    "    'n_estimators': range(5, 50, 5),\n",
    "    'max_depth': [2, 5, 10, -1],\n",
    "    'learning_rate': [.4, .5, .6],\n",
    "    'colsample_bytree': [.6, .7, .8, .9, 1]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "xgb_gs = GridSearchCV(\n",
    "    estimator=xgb_model, \n",
    "    param_grid=gbm_param_grid,\n",
    "    scoring=\"roc_auc\",\n",
    "    verbose=1,\n",
    "    cv=3,\n",
    "    n_jobs=-2\n",
    ")\n",
    "\n",
    "# Fit grid search to the data\n",
    "xgb_gs.fit(x_train, train_Y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", xgb_gs.best_params_)\n",
    "print(\"Best ROC-AUC found: \", xgb_gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best = XGBClassifier(**xgb_gs.best_params_)\n",
    "xgb_best.fit(x_train, train_Y)\n",
    "\n",
    "print(\"Test ROC-AUC: \", metrics.roc_auc_score(xgb_best.predict(x_test), test_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
